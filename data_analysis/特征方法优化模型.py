'''
随机森林的特征选择法：——Gini Importance
原理：
    使用Gini指数表示节点的纯度，Gini指数越大，纯度越低。然后计算每个节点的Gini指数--子节点的Gini指数之和，记做Gini decrease.最后将所有树
    上相同那个特征节点的Gini decrease加权的和记为Gini importance.该数值会在0-1之间，该数值越大，即代表该节点（特征）重要性越大。
参数计算：
    Gini index:衡量决策树每一棵树上的节点上面所存在的数据的纯净度的一个指标，这个值越小，纯净度越高。
    公式：Gini(p) = sum(pi*(1-pi)) = 1-sum(pi^2);pi是节点内各个特征所占的概率
    Gini decrease:每个节点的Gini indeex --子节点的Gini index之和（这里的和是加权和）
    Gini importance:将所有树上相同特征节点的Gini decrease加权的和。

注意：
    1）这种方法存在偏向，对具有更多取值的特征会更有利
    2）对于存在关联的多个特征，其中任意一个都可以作为指示器（优秀的特征），并且一旦某个特征被选择之后，其他特征的重要度就会急剧下降，因为不纯度已经被
    选中的那个特征降下来了，其他的特征就很难在降低那么多的不纯度了，这样一来，只有先被选中的那个特征重要度很高，其他的关联特征重要度往往较低，在理解
    数据时，这就会造成误解，导致错误的认为先被选中的恩正很重要，其余的特征不重要。但实际上这些特征对响应变量的作用确实非常接近。
'''

